{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refinement in the windows \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "COCO_API_DIR = 'D:/Datasets/COCO/cocoapi-master/PythonAPI'\n",
    "COCO_ROOT = 'D:/Datasets/COCO/'\n",
    "COCO_SET = 'train2017'\n",
    "import sys\n",
    "import threading\n",
    "from threading import Thread\n",
    "import time\n",
    "sys.path.append('../frontend')\n",
    "sys.path.append('../backend')\n",
    "sys.path.append(COCO_API_DIR)\n",
    "\n",
    "import cv2\n",
    "import datetime\n",
    "import pickle\n",
    "import json\n",
    "import time\n",
    "import glob\n",
    "import timeit\n",
    "import skimage.io as io\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "from keras.layers import *\n",
    "import keras\n",
    "from keras.models import *\n",
    "from keras import losses\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.optimizers import *\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "from pycocotools.coco import COCO\n",
    "import random\n",
    "import datasets\n",
    "import composition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=17.79s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=9.84s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "annFile='{}/annotations/instances_{}.json'.format(COCO_ROOT,COCO_SET)\n",
    "annFileKPS = '{}/annotations/person_keypoints_{}.json'.format(COCO_ROOT,COCO_SET)\n",
    "catNms = ['person']\n",
    "coco=COCO(annFile)\n",
    "coco_kps=COCO(annFileKPS)  \n",
    "catIds = coco.getCatIds(catNms)\n",
    "imgIds = coco.getImgIds(catIds=catIds )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the real examples threadStarting the fake examples thread\n",
      "\n",
      "Real examples done\n",
      "Fake examples done\n"
     ]
    }
   ],
   "source": [
    "def get_real_examples(list_out, coco, coco_kps, coco_root, imgIds, catIds, params, batch_size):\n",
    "    print('Starting the real examples thread')\n",
    "    # GET REFERENCE, NATURAL LOOKING EXAMPLES\n",
    "    for i in range(0, batch_size):\n",
    "        IM = composition.get_image_suitable_for_blending_with_meta(coco, coco_kps, coco_root, imgIds, catIds, longer_edge_size=params['longer_edge_size'], scale_range=params['scale_range_A'], shift_range=params['shift_range_A'], rot_range=params['rot_range_A'], allow_overlaps=params['allow_overlaps_in_A'])\n",
    "        list_out.append(IM)\n",
    "    print('Real examples done')\n",
    "\n",
    "def get_fake_examples(list_out, coco, coco_kps, coco_root, imgIds, catIds, params, batch_size):\n",
    "    print('Starting the fake examples thread')\n",
    "    # GET GENERATED EXAMPLES\n",
    "    for i in range(0, batch_size):\n",
    "        while True:\n",
    "            data_train = composition.get_composition(coco, coco_kps,coco_root, imgIds, catIds, params=params)\n",
    "            if data_train is not None:\n",
    "                list_out.append(data_train)\n",
    "                break\n",
    "    print('Fake examples done')\n",
    "    \n",
    "\n",
    "def get_training_examples(coco, coco_kps, coco_root, imgIds, catIds, composition_params, batch_size):\n",
    "    \n",
    "    list_real = list()\n",
    "    list_fake = list()\n",
    "    thread_real = Thread(target=get_real_examples, args=(list_real, coco, coco_kps, coco_root, imgIds, catIds, composition_params, batch_size))\n",
    "    thread_fake = Thread(target=get_fake_examples, args=(list_fake, coco, coco_kps, coco_root, imgIds, catIds, composition_params, batch_size))\n",
    "    \n",
    "    thread_real.start()\n",
    "    thread_fake.start()\n",
    "\n",
    "    thread_real.join()\n",
    "    thread_fake.join()\n",
    "    return list_real, list_fake\n",
    "\n",
    "# Get initial parameters\n",
    "batch_size = 2\n",
    "train_data = get_training_examples(coco, coco_kps, COCO_ROOT, imgIds, catIds, composition.default_composition_params, batch_size)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         (None, 255, 255, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 255, 255, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 255, 255, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 127, 127, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 127, 127, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 127, 127, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 63, 63, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 63, 63, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 63, 63, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 63, 63, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv4 (Conv2D)        (None, 63, 63, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 31, 31, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 31, 31, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 31, 31, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 31, 31, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv4 (Conv2D)        (None, 31, 31, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 15, 15, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 15, 15, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 15, 15, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 15, 15, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv4 (Conv2D)        (None, 15, 15, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "=================================================================\n",
      "Total params: 20,024,384\n",
      "Trainable params: 20,024,384\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'Tensor' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-d606e70078de>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mvgg_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvgg_front\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvgg_input\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvgg_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'Tensor' object is not callable"
     ]
    }
   ],
   "source": [
    "# \n",
    "vgg_front = keras.applications.vgg19.VGG19(\n",
    "    include_top=False, \n",
    "    weights='imagenet', \n",
    "    input_tensor=None, \n",
    "    input_shape=(255,255,3), \n",
    "    pooling=None)\n",
    "vgg_front.summary()\n",
    "\n",
    "vgg_input = vgg_front.inputs[0]\n",
    "x = vgg_front(vgg_input)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# LET'S BUILD A MODEL NOW\n",
    "def stage_block(features, \n",
    "                prev_block=None, \n",
    "                stage_name='KD', \n",
    "                num_kernels_per_step=128, \n",
    "                num_kernels_out=128,\n",
    "                num_convs=7, \n",
    "                padding='same',\n",
    "               prefix=''):\n",
    "    \n",
    "    if prev_block is not None:\n",
    "        x = concatenate([features, prev_block],3)\n",
    "    else:\n",
    "        x = features\n",
    "    for i in range(0, num_convs-2):\n",
    "        x = Conv2D(num_kernels_per_step, \n",
    "                   (7,7), \n",
    "                   strides=(1,1), \n",
    "                   padding=padding, \n",
    "                   kernel_initializer='he_uniform', \n",
    "                   name='{}Mconv{}_stage{}'.format(prefix,i, stage_name), activation='relu')(x)\n",
    "        \n",
    "    x = Conv2D(num_kernels_per_step, (1,1), strides=(1,1), padding='valid', kernel_initializer='he_uniform', name='{}Mconv{}_stage{}'.format(prefix,num_convs-1,stage_name), activation='relu')(x)\n",
    "    x = Conv2D(num_kernels_out, (1,1), strides=(1,1), padding='valid',kernel_initializer='he_uniform', name='{}Mconv{}_stage{}'.format(prefix,num_convs,stage_name))(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "# DISCRIMINATOR - REAL VS FAKE\n",
    "dis_input = Input((256, 256, 3))\n",
    "x = dis_input\n",
    "for i in range(0,7):\n",
    "    x = stage_block(x, num_convs=3, prefix='dsx_{}_'.format(i))\n",
    "    x = MaxPool2D((2,2))(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(1, kernel_initializer='he_uniform')(x)\n",
    "dis_model = Model(dis_input, x)\n",
    "\n",
    "dis_model.trainable = False\n",
    "\n",
    "# REFINERS and FULL MODELS\n",
    "ref_models = []\n",
    "full_models = []\n",
    "num_refiners = 9\n",
    "num_kernels_per_step = 32\n",
    "ref_input = Input((256,256,11)) # 3 + 3 +3 + 2, A, B, C, mask A, mask B \n",
    "for i in range(0, num_refiners):\n",
    "    x = Conv2D(num_kernels_per_step, (3,3), strides=(1,1),  kernel_initializer='he_uniform', padding='same')(ref_input)\n",
    "    x = LeakyReLU(0.1)(x)\n",
    "    for d in range(0, i):\n",
    "        x = Conv2D(num_kernels_per_step, (3,3), strides=(1,1),  kernel_initializer='he_uniform', padding='same')(x)\n",
    "        x = LeakyReLU(0.1)(x)\n",
    "        x = Conv2D(num_kernels_per_step, (3,3), strides=(1,1),  kernel_initializer='he_uniform', padding='same')(x)\n",
    "        x = LeakyReLU(0.1)(x)\n",
    "    x = Conv2D(3, (3,3), strides=(1,1), padding='same',  kernel_initializer='he_uniform')(x)\n",
    "    ref_output = x\n",
    "    ref_model = Model(ref_input, ref_output)\n",
    "    ref_models.append(ref_model)\n",
    "    \n",
    "    full_output = dis_model( ref_output )\n",
    "    full_model = Model(ref_input, full_output)\n",
    "    full_models.append(full_model)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('FULL MODELS\\n-----------------------------')\n",
    "dis_model.trainable = False\n",
    "for i,full_model in enumerate(full_models):\n",
    "    print('FULL MODEL {}'.format(i))\n",
    "    full_opt = optimizers.Adam(lr=0.00005)\n",
    "    full_model.compile(full_opt, 'mean_absolute_error')\n",
    "    full_model.summary()\n",
    "\n",
    "print('DISCRIMINATOR')\n",
    "dis_model.trainable = True\n",
    "dis_opt = optimizers.Adam(lr=0.00005)\n",
    "dis_model.compile(dis_opt, 'mean_absolute_error')\n",
    "dis_model.summary()\n",
    "\n",
    "print('REFINERS\\n------------------------------')\n",
    "for i,ref_model in enumerate(ref_models):\n",
    "    print('REFINER {}'.format(i))\n",
    "    ref_opt = optimizers.Adam(lr=0.00001)\n",
    "    ref_model.compile(ref_opt, 'mean_absolute_error')\n",
    "    ref_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def visualize_progress(progress):\n",
    "    \n",
    "    if len(progress) < 3:\n",
    "        return\n",
    "    \n",
    "    labels = []\n",
    "    x = np.array([ progress[i][0] for i in range(0, len(progress)) ])\n",
    "    \n",
    "    gen_dis_loss = np.array([ progress[i][1] for i in range(0, len(progress)) ])\n",
    "    labels.append('Gen Dis')\n",
    "    \n",
    "    dis_losses = np.array([ progress[i][2] for i in range(0, len(progress)) ])\n",
    "    for i in range(0,dis_losses.shape[1]):\n",
    "        labels.append('Dis {}'.format(i))\n",
    "\n",
    "    ref_losses = np.array([ progress[i][3] for i in range(0, len(progress)) ])\n",
    "    for i in range(0,ref_losses.shape[1]):\n",
    "        labels.append('Ref {}'.format(i))\n",
    "\n",
    "        \n",
    "    full_losses = np.array([ progress[i][4] for i in range(0, len(progress)) ])\n",
    "    for i in range(0,full_losses.shape[1]):\n",
    "        labels.append('Full {}'.format(i))\n",
    "\n",
    "        \n",
    "    # TRIMMING\n",
    "    histlen = 100\n",
    "    x = x[ np.max([0, x.shape[0]-histlen]):, ]\n",
    "    gen_dis_loss = gen_dis_loss[ np.max([0, gen_dis_loss.shape[0]-histlen]):, ]\n",
    "    dis_losses = dis_losses[ np.max([0, dis_losses.shape[0]-histlen]):, ]        \n",
    "    ref_losses = ref_losses[ np.max([0, ref_losses.shape[0]-histlen]):, ]        \n",
    "    full_losses = full_losses[ np.max([0, full_losses.shape[0]-histlen]):, ]        \n",
    "        \n",
    "    print('Last dis losses: {}'.format(dis_losses[-1,:]))\n",
    "    print('Last full losses: {}'.format(full_losses[-1,:]))\n",
    "        \n",
    "    plt.figure(figsize=(12,5))\n",
    "    plt.plot(x ,gen_dis_loss)\n",
    "    for i in range(0, dis_losses.shape[1]):\n",
    "        plt.plot(x, dis_losses[:,i])\n",
    "    for i in range(0, full_losses.shape[1]):\n",
    "        plt.plot(x, full_losses[:,i])\n",
    "    plt.legend(labels)\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "def show_ref_viz(ref_viz):\n",
    "    img = np.zeros((256, 256*len(ref_viz), 3))\n",
    "    img[:, :256,:] = ref_viz[0][:,:,0:3]\n",
    "    for i in range(1, len(ref_viz)):\n",
    "        img[:, i*256:(i+1)*256, :] = np.clip(ref_viz[i], 0, 255)\n",
    "        \n",
    "    plt.figure(figsize=(20,7))\n",
    "    plt.imshow(img.astype(np.uint8))\n",
    "    plt.show()\n",
    "    return img.astype(np.uint8)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = 0\n",
    "progress = []\n",
    "composition_params = composition.default_composition_params\n",
    "composition_params['longer_edge_size'] = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "while True:\n",
    "\n",
    "    # Set up the threads to work in the background while running GPU tasks\n",
    "    list_real = list()\n",
    "    list_fake = list()\n",
    "    thread_real = Thread(target=get_real_examples, args=(list_real, coco, coco_kps, COCO_ROOT, imgIds, catIds, composition.default_composition_params, batch_size))\n",
    "    thread_fake = Thread(target=get_fake_examples, args=(list_fake, coco, coco_kps, COCO_ROOT, imgIds, catIds, composition.default_composition_params, batch_size))\n",
    "    \n",
    "    thread_real.start()\n",
    "    thread_fake.start()\n",
    "    \n",
    "    # ONLY IF WE HAVE THE EXAMPLES, THUS BEING ITERATION NUMBER > 0\n",
    "    if it > 0:\n",
    "        # TRAIN DISCRIMINATOR FIRST ON REAL VS STANDARD COMPOSITION\n",
    "        print('General discriminator training')\n",
    "        dis_model.trainable = True\n",
    "        gen_dis_loss_cur = dis_model.train_on_batch(dis_x, dis_y)\n",
    "\n",
    "        # TRAIN THE FULL MODELS\n",
    "        dis_losses = [0]*num_refiners\n",
    "        ref_losses = [0]*num_refiners\n",
    "        full_losses = [0]*num_refiners\n",
    "        for i in range(0, num_refiners):\n",
    "            print('Discriminator training for predictions from ref {}'.format(i))\n",
    "            dis_model.trainable = True\n",
    "            # Get prediction from particular refiner\n",
    "            dis_x_fake_pred = ref_models[i].predict( ref_x[batch_size//2:,] )\n",
    "            \n",
    "            dis_loss_cur = dis_model.train_on_batch(np.concatenate([ dis_x_real,dis_x_fake_pred ],0), dis_y)\n",
    "            dis_losses[i] = dis_loss_cur\n",
    "            \n",
    "            # Train in AE task\n",
    "            if 0:\n",
    "                print('Refiner {} training'.format(i))\n",
    "                ref_batch_x = ref_x\n",
    "                ref_batch_y = ref_x[:,:,:,0:3]\n",
    "                ref_loss_cur = ref_models[i].train_on_batch(ref_batch_x, ref_batch_y)\n",
    "                ref_losses[i] = ref_loss_cur\n",
    "            ref_losses[i] = 0 \n",
    "            \n",
    "            # Train the whole model next\n",
    "            print('Full {} training'.format(i))\n",
    "            dis_model.trainable = False\n",
    "            full_batch_x = ref_x\n",
    "            full_batch_y = np.ones((batch_size, 1))\n",
    "            full_loss_cur = full_models[i].train_on_batch(full_batch_x, full_batch_y)\n",
    "            full_losses[i] = full_loss_cur\n",
    "        print('Dis losses: {}'.format(dis_losses))\n",
    "        print('Full losees: {}'.format(full_losses))\n",
    "        \n",
    "        progress.append([ it, gen_dis_loss_cur, dis_losses, ref_losses, full_losses ])\n",
    "        \n",
    "    it += 1\n",
    "    \n",
    "    # Wait for the threads to stop and store new training data\n",
    "    thread_real.join()\n",
    "    thread_fake.join()\n",
    "    \n",
    "    \n",
    "    \n",
    "    # PREP THE TRAINING DATA\n",
    "    ref_x = np.array([ np.concatenate([ list_fake[i]['image_scaled_and_padded'],\n",
    "                                    list_fake[i]['A']['image_scaled_and_padded'],\n",
    "                                   list_fake[i]['B']['image_scaled_and_padded'],\n",
    "                                   np.expand_dims(list_fake[i]['A']['composite_mask'],2),\n",
    "                                     np.expand_dims(list_fake[i]['inserted_mask'], 2)\n",
    "                                  ], 2) for i in range(0,batch_size) ])\n",
    "\n",
    "    dis_x_real = np.array([ list_real[i]['image_scaled_and_padded'] for i in range(0, batch_size//2) ])\n",
    "    dis_x_fake = np.array([ list_fake[i]['image_scaled_and_padded'] for i in range(0, batch_size//2) ])\n",
    "    dis_x = np.concatenate([ dis_x_real, dis_x_fake ], 0)\n",
    "    dis_y = np.zeros((batch_size,1))\n",
    "    dis_y[0:batch_size//2,0] = 1\n",
    "    \n",
    "    \n",
    "    if it % 2 == 0 and it > 2:\n",
    "        \n",
    "        ref_viz = [ ref_x[0] ]\n",
    "        for i in range(0, num_refiners):\n",
    "            dis_x_fake_pred = ref_models[i].predict( ref_x[0:1] )\n",
    "            ref_viz.append( dis_x_fake_pred[0] )\n",
    "            \n",
    "        clear_output()\n",
    "        img = show_ref_viz(ref_viz)\n",
    "        visualize_progress(progress)\n",
    "        try:\n",
    "            cv2.imwrite('current_progress.png', cv2.cvtColor(img, cv2.COLOR_RGB2BGR))\n",
    "        except:\n",
    "            print('For some reason unable to write a file...')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
